{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Local Learning Rules to perform simple tasks in a Feedforward Pytorch Network\n",
    "Code by Rishika Mohanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement a simple toy feedforward network with simple learning rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy input for AND problem\n",
    "input = torch.tensor([[-1,-1],[-1,1],[1,-1],[1,1]], dtype=torch.float)\n",
    "output = torch.tensor([[-1], [-1], [-1], [1]], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import FeedForwardNetwork, FeedForwardLearningRule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Feed-Forward Neural Network with arbitrary number of hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input : int, hidden_layer_sizes : list, n_output: int):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "        :param n_input: number of input neurons\n",
    "        :param hidden_layer_sizes: list of number of hidden neurons in each layer\n",
    "        :param n_output: number of output neurons\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(n_input, hidden_layer_sizes[0], bias = False)]) # first layer\n",
    "        self.layers.extend([nn.Linear(h1, h2, bias = False) for h1, h2 in zip(hidden_layer_sizes, hidden_layer_sizes[1:])]) # hidden layers\n",
    "        self.layers.append(nn.Linear(hidden_layer_sizes[-1], n_output, bias = False)) # output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        :param x: network input\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = torch.sigmoid(layer(x)) # apply ReLU activation function\n",
    "        self.hebbian_update(x,0.1)\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reset the parameters of the network\n",
    "        \"\"\"\n",
    "        for layer in self.layers: # reset all layers\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "    def hebbian_forward(self,input,lr,max_weight=1):\n",
    "        \"\"\"\n",
    "        Perform forward Hebbian update of the network\n",
    "        :param input: input to the network\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            y = torch.sigmoid(layer(x))\n",
    "            if max_weight is not None:\n",
    "                layer.weight.data += lr * torch.mm(y.t(), x) * (max_weight - torch.abs(layer.weight.data))\n",
    "            else:\n",
    "                layer.weight.data += lr * torch.mm(y.t(), x)\n",
    "            x = y\n",
    "        \n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get the weights of the network\n",
    "        \"\"\"\n",
    "        return [layer.weight.data.numpy() for layer in self.layers]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Set the weights of the network\n",
    "        :param weights: list of weights\n",
    "        \"\"\"\n",
    "        for layer, weight in zip(self.layers, weights):\n",
    "            layer.weight.data = torch.from_numpy(weight)\n",
    "\n",
    "    def arbitrary_forward(self, input, synapse_updater):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the network using an arbitrary update function\n",
    "        :param input: input to the network\n",
    "        :param synapse_update_function: function that updates the weights and biases of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            y = torch.sigmoid(layer(x))\n",
    "            for i in range(y.shape[1]):\n",
    "                for j in range(x.shape[1]):\n",
    "\n",
    "                    layer.weight.data[i,j] = synapse_updater(torch.cat([x[:,j],y[:,i],layer.weight.data[i,j].view(1)]))\n",
    "            x = y\n",
    "\n",
    "class FeedForwardLearningRule(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements an arbitrary local memoryless learning rule\n",
    "    \"\"\"\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Initialize the learning rule\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = 2\n",
    "        self.output_size = 1\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        :param x: network input\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = FeedForwardNetwork(2, [5,10], 1)\n",
    "init_weights = temp.get_weights()\n",
    "with torch.no_grad():\n",
    "    temp.reset_parameters()\n",
    "    input_tensor = []\n",
    "    output_tensor = []\n",
    "    for steps in range(3):\n",
    "        random_input = torch.randn(1,2)\n",
    "        output = temp.forward(random_input)\n",
    "        temp.hebbian_update(random_input, 0.1)\n",
    "        input_tensor.append(random_input)\n",
    "        output_tensor.append(output)\n",
    "    input_tensor = torch.cat(input_tensor)\n",
    "    output_tensor = torch.cat(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = FeedForwardNetwork(2, [5,10], 1)\n",
    "# temp.set_weights(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4087]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(input_tensor[0].view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [5,5]\n",
    "shared_layers = nn.ModuleList([nn.Linear(3, hidden_layer_sizes[0], bias = False)])\n",
    "shared_layers.extend([nn.Linear(h1, h2, bias = False) for h1, h2 in zip(hidden_layer_sizes, hidden_layer_sizes[1:])])\n",
    "shared_layers.append(nn.Linear(hidden_layer_sizes[-1], 1, bias = False))\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(temp.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = temp(input_tensor)\n",
    "    loss = loss_function(output, output_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "temp.arbitrary_update(input_tensor[0].view(1,-1),FeedForwardLearningRule(shared_layers))\n",
    "temp(input_tensor[1].view(1,-1))\n",
    "loss = loss_function(temp(input_tensor[1].view(1,-1)), output_tensor[1].view(1,-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Implement a Neural Network with an arbitrary synaptic learning rule\n",
    "# class ToyNetwork(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Simple Feed-Forward Neural Network with arbitrary number of hidden layers and no biases\n",
    "#     \"\"\"\n",
    "#     def __init__(self, n_input : int, hidden_layer_sizes : list, n_output: int):\n",
    "#         \"\"\"\n",
    "#         Initialize the network\n",
    "#         :param n_input: number of input neurons\n",
    "#         :param hidden_layer_sizes: list of number of hidden neurons in each layer\n",
    "#         :param n_output: number of output neurons\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([nn.Linear(n_input, hidden_layer_sizes[0],bias=False)]) # first layer\n",
    "#         self.layers.extend([nn.Linear(h1, h2, bias=False) for h1, h2 in zip(hidden_layer_sizes, hidden_layer_sizes[1:])]) # hidden layers\n",
    "#         self.layers.append(nn.Linear(hidden_layer_sizes[-1], n_output, bias=False)) # output layer\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the network\n",
    "#         :param x: network input\n",
    "#         \"\"\"\n",
    "#         for layer in self.layers[:-1]: # skip the last layer\n",
    "#             x = torch.relu(layer(x)) # apply ReLU activation function\n",
    "#         x = torch.tanh(self.layers[-1](x)) # apply tanh activation function to have output in [-1,1]\n",
    "#         return x\n",
    "    \n",
    "#     def reset_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Reset the parameters of the network\n",
    "#         \"\"\"\n",
    "#         for layer in self.layers: # reset all layers\n",
    "#             layer.reset_parameters()\n",
    "    \n",
    "#     def supervised_hebbian_update(self, input, output, learning_rate):\n",
    "#         \"\"\"\n",
    "#         Perform a Hebbian update on the network in the backward direction given the output and input\n",
    "#         :param input: input to the network\n",
    "#         :param output: target output\n",
    "#         :param learning_rate: learning rate\n",
    "#         \"\"\"\n",
    "#         y = output # target output\n",
    "#         for layer_no in range(len(self.layers)-1, 0, -1): # go through all layers in reverse order\n",
    "#             # forward pass through the network up to before the current layer\n",
    "#             x = input\n",
    "#             for layer in self.layers[:layer_no]: # go through all layers up to the current layer\n",
    "#                 x = torch.relu(layer(x)) # apply ReLU activation function\n",
    "            \n",
    "#             deltaw = learning_rate * torch.mm(y.t(), x) # calculate Hebbian update\n",
    "#             self.layers[layer_no].weight.data += deltaw # update weights\n",
    "#             self.layers[layer_no].weight.data = torch.clamp(self.layers[layer_no].weight.data, min=-1, max=1) # clip weights to [-1,1]\n",
    "#             y = x # update target output\n",
    "    \n",
    "#     def unsupervised_hebbian_update(self, input, learning_rate):\n",
    "#         \"\"\"\n",
    "#         Perform a Hebbian update on the network in the forward direction given only the input\n",
    "#         :param input: input to the network\n",
    "#         :param learning_rate: learning rate\n",
    "#         \"\"\"\n",
    "#         x = input # start with input to the network\n",
    "#         for layer in self.layers: # go through all layers\n",
    "#             y = layer(x) # forward step through the network\n",
    "#             deltaw = learning_rate * x.t().mm(y).t() # calculate Hebbian update\n",
    "#             layer.weight.data += deltaw # update weights\n",
    "#             layer.weight.data  = layer.weight.data.clamp(-1, 1) # clip weights to [-1,1]\n",
    "#             x = layer(x) # current output becomes input for the next layer\n",
    "    \n",
    "#     def semisupervised_hebbian_update(self, input, output, learning_rate):\n",
    "#         \"\"\"\n",
    "#         Perform a Hebbian update on the network in the forward direction given the input but some information about error is available as a global context\n",
    "#         :param input: input to the network\n",
    "#         :param output: target output\n",
    "#         :param learning_rate: learning rate\n",
    "#         \"\"\"\n",
    "#         binarized_prediction = (self.forward(input)>0).float()\n",
    "#         binarized_target = (output>0).float()\n",
    "#         loss = F.mse_loss(binarized_prediction, binarized_target) # calculate MSE loss\n",
    "        \n",
    "#         global_context = loss.item() \n",
    "#         # Interpretation: 0 if correct, 1 if incorrect\n",
    "#         # Scale to be consistent with the sign convention of the Hebbian update\n",
    "#         global_context = 1-2*global_context\n",
    "#         # Interpretation: -1 if incorrect, 1 if correct\n",
    "#         # Condition the sign of the global reward on the state of the network\n",
    "#         global_context = -(2*binarized_prediction-1)*global_context\n",
    "#         # Interpretation: \n",
    "#         # -1 if incorrect, 1 if correct (neuron is active); \n",
    "#         # -1 if correct, 1 if incorrect (both inactive)\n",
    "\n",
    "#         x = input\n",
    "#         for layer in self.layers:\n",
    "#             y = layer(x)\n",
    "#             deltaw = learning_rate * x.t().mm(y).t() * global_context\n",
    "#             layer.weight.data += deltaw\n",
    "#             layer.weight.data  = layer.weight.data.clamp(-1, 1)\n",
    "#             x = y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebc1b3327c947d8a4790b77ea19a1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.73\n"
     ]
    }
   ],
   "source": [
    "n_replications = 100 # number of replications to estimate the average performance\n",
    "print_every = None\n",
    "hidden_layer_sizes = [10] # number of hidden neurons in each layer\n",
    "\n",
    "\n",
    "success_rate = [] \n",
    "for _ in tqdm(range(n_replications)): # run the weight training\n",
    "    model = ToyNetwork(2, hidden_layer_sizes, 1) # initialize the network\n",
    "    for iters in range(400): # train the network for 400 iterations\n",
    "        random_index = np.random.randint(0, 4) # randomly select an example from the dataset to train on\n",
    "        model.supervised_hebbian_update(input[random_index].unsqueeze(0), output[random_index].unsqueeze(0), 0.01) # perform a Hebbian update\n",
    "        if print_every is not None and iters % print_every == 0: \n",
    "            out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "            print(f'Iteration {iters}: XOR = {out}')\n",
    "    \n",
    "    out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "    expected_out = (output>0).float().detach().numpy().T[0] # get the expected output of the network\n",
    "    if np.all(out == expected_out):\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "success_rate = np.mean(success_rate)\n",
    "print(f'Success rate: {success_rate}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aca96c5d62140dc9a5b5ec718896b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.98\n"
     ]
    }
   ],
   "source": [
    "n_replications = 100 # number of replications to estimate the average performance\n",
    "print_every = None\n",
    "hidden_layer_sizes = [100] # number of hidden neurons in each layer\n",
    "\n",
    "\n",
    "success_rate = [] \n",
    "for _ in tqdm(range(n_replications)): # run the weight training\n",
    "    model = ToyNetwork(2, hidden_layer_sizes, 1) # initialize the network\n",
    "    for iters in range(400): # train the network for 400 iterations\n",
    "        random_index = np.random.randint(0, 4) # randomly select an example from the dataset to train on\n",
    "        model.supervised_hebbian_update(input[random_index].unsqueeze(0), output[random_index].unsqueeze(0), 0.01) # perform a Hebbian update\n",
    "        if print_every is not None and iters % print_every == 0: \n",
    "            out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "            print(f'Iteration {iters}: XOR = {out}')\n",
    "    \n",
    "    out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "    expected_out = (output>0).float().detach().numpy().T[0] # get the expected output of the network\n",
    "    if np.all(out == expected_out):\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "success_rate = np.mean(success_rate)\n",
    "print(f'Success rate: {success_rate}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcc7e5531614dc1b74b38509823f041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_replications = 100 # number of replications to estimate the average performance\n",
    "print_every = None\n",
    "hidden_layer_sizes = [10,10] # number of hidden neurons in each layer\n",
    "\n",
    "\n",
    "success_rate = [] \n",
    "for _ in tqdm(range(n_replications)): # run the weight training\n",
    "    model = ToyNetwork(2, hidden_layer_sizes, 1) # initialize the network\n",
    "    for iters in range(1000): # train the network for 400 iterations\n",
    "        random_index = np.random.randint(0, 4) # randomly select an example from the dataset to train on\n",
    "        model.supervised_hebbian_update(input[random_index].unsqueeze(0), output[random_index].unsqueeze(0), 0.01) # perform a Hebbian update\n",
    "        if print_every is not None and iters % print_every == 0: \n",
    "            out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "            print(f'Iteration {iters}: XOR = {out}')\n",
    "    \n",
    "    out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "    expected_out = (output>0).float().detach().numpy().T[0] # get the expected output of the network\n",
    "    if np.all(out == expected_out):\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "success_rate = np.mean(success_rate)\n",
    "print(f'Success rate: {success_rate}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy input for XOR problem\n",
    "input = torch.tensor([[-1,-1],[-1,1],[1,-1],[1,1]], dtype=torch.float)\n",
    "output = torch.tensor([[-1], [1], [1], [-1]], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967460fd71be4d679f53a2b9c827b984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.29\n"
     ]
    }
   ],
   "source": [
    "n_replications = 100 # number of replications to estimate the average performance\n",
    "print_every = None\n",
    "hidden_layer_sizes = [10] # number of hidden neurons in each layer\n",
    "\n",
    "\n",
    "success_rate = [] \n",
    "for _ in tqdm(range(n_replications)): # run the weight training\n",
    "    model = ToyNetwork(2, hidden_layer_sizes, 1) # initialize the network\n",
    "    for iters in range(400): # train the network for 400 iterations\n",
    "        random_index = np.random.randint(0, 4) # randomly select an example from the dataset to train on\n",
    "        model.supervised_hebbian_update(input[random_index].unsqueeze(0), output[random_index].unsqueeze(0), 0.01) # perform a Hebbian update\n",
    "        if print_every is not None and iters % print_every == 0: \n",
    "            out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "            print(f'Iteration {iters}: XOR = {out}')\n",
    "    \n",
    "    out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "    expected_out = (output>0).float().detach().numpy().T[0] # get the expected output of the network\n",
    "    if np.all(out == expected_out):\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "success_rate = np.mean(success_rate)\n",
    "print(f'Success rate: {success_rate}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc7f6f1e3fe4685b8a562d1d87a9b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_replications = 100 # number of replications to estimate the average performance\n",
    "print_every = None\n",
    "hidden_layer_sizes = [10,10] # number of hidden neurons in each layer\n",
    "\n",
    "\n",
    "success_rate = [] \n",
    "for _ in tqdm(range(n_replications)): # run the weight training\n",
    "    model = ToyNetwork(2, hidden_layer_sizes, 1) # initialize the network\n",
    "    for iters in range(400): # train the network for 400 iterations\n",
    "        random_index = np.random.randint(0, 4) # randomly select an example from the dataset to train on\n",
    "        model.supervised_hebbian_update(input[random_index].unsqueeze(0), output[random_index].unsqueeze(0), 0.01) # perform a Hebbian update\n",
    "        if print_every is not None and iters % print_every == 0: \n",
    "            out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "            print(f'Iteration {iters}: XOR = {out}')\n",
    "    \n",
    "    out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "    expected_out = (output>0).float().detach().numpy().T[0] # get the expected output of the network\n",
    "    if np.all(out == expected_out):\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "success_rate = np.mean(success_rate)\n",
    "print(f'Success rate: {success_rate}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51561e5292624df093761689ad55df9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.99\n"
     ]
    }
   ],
   "source": [
    "n_replications = 100 # number of replications to estimate the average performance\n",
    "print_every = None\n",
    "hidden_layer_sizes = [100] # number of hidden neurons in each layer\n",
    "\n",
    "\n",
    "success_rate = [] \n",
    "for _ in tqdm(range(n_replications)): # run the weight training\n",
    "    model = ToyNetwork(2, hidden_layer_sizes, 1) # initialize the network\n",
    "    for iters in range(400): # train the network for 400 iterations\n",
    "        random_index = np.random.randint(0, 4) # randomly select an example from the dataset to train on\n",
    "        model.supervised_hebbian_update(input[random_index].unsqueeze(0), output[random_index].unsqueeze(0), 0.01) # perform a Hebbian update\n",
    "        if print_every is not None and iters % print_every == 0: \n",
    "            out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "            print(f'Iteration {iters}: XOR = {out}')\n",
    "    \n",
    "    out = (model.forward(input)>0).float().detach().numpy().T[0] # get the output of the network\n",
    "    expected_out = (output>0).float().detach().numpy().T[0] # get the expected output of the network\n",
    "    if np.all(out == expected_out):\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "success_rate = np.mean(success_rate)\n",
    "print(f'Success rate: {success_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ToyNetwork(2, [100,100], 1)\n",
    "# print_every = 10\n",
    "# for iters in range(200):\n",
    "#     model.hebbian_update(input, output, 0.01)\n",
    "#     if iters % print_every == 0:\n",
    "#         print(f'Iteration {iters}: XOR = {model.forward(input).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LSTM that predicts the next change in synaptic weight locally\n",
    "class weightUpdater(nn.Module):\n",
    "    def __init__(self, lstm, fc):\n",
    "        super(weightUpdater, self).__init__()\n",
    "        self.lstm = lstm\n",
    "        self.fc = fc\n",
    "        self.input_size = lstm.input_size\n",
    "        self.hidden_size = lstm.hidden_size\n",
    "        self.output_size = fc.out_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,_ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_lstm = nn.LSTM(2, 10, 1)\n",
    "shared_fc = nn.Linear(10, 1)\n",
    "\n",
    "synapse = weightUpdater(shared_lstm, shared_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2215],\n",
       "         [-0.2007],\n",
       "         [-0.2007]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapse(torch.tensor([[[0.,1.],[1.,0.],[1.,0.]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bc6635aead129ae5e44b7477cd2f864a11fdcb46f1249fa60a026af62e8401f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('flymazerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
